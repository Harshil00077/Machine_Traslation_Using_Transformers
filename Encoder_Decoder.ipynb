{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-13T16:21:24.388868Z","iopub.execute_input":"2024-06-13T16:21:24.390136Z","iopub.status.idle":"2024-06-13T16:21:24.399576Z","shell.execute_reply.started":"2024-06-13T16:21:24.390089Z","shell.execute_reply":"2024-06-13T16:21:24.398122Z"},"trusted":true},"execution_count":248,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\nimport numpy as np\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:21:24.401927Z","iopub.execute_input":"2024-06-13T16:21:24.402654Z","iopub.status.idle":"2024-06-13T16:21:24.414320Z","shell.execute_reply.started":"2024-06-13T16:21:24.402611Z","shell.execute_reply":"2024-06-13T16:21:24.412684Z"},"trusted":true},"execution_count":249,"outputs":[]},{"cell_type":"code","source":"def scaled_dot_product(q, k, v, mask=None):\n    # q, k ,v = 30 x 8 x 200 x 64\n    d_k = q.shape[-1] # 64\n    scaled = torch.matmul(q, k.transpose(-1,-2))//math.sqrt(d_k) # 30 x 8 x 200 x 200\n    if mask is not None:\n        scaled += mask  # 30 x 8 x 200 x 200\n    \n    attention = F.softmax(scaled, dim=-1) # 30 x 8 x 200 x 200\n    values = torch.matmul(attention, v) # 30 x 8 x 200 x 64\n    return values,attention ","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:21:24.416241Z","iopub.execute_input":"2024-06-13T16:21:24.416749Z","iopub.status.idle":"2024-06-13T16:21:24.427866Z","shell.execute_reply.started":"2024-06-13T16:21:24.416655Z","shell.execute_reply":"2024-06-13T16:21:24.426270Z"},"trusted":true},"execution_count":250,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model # 512\n        self.num_heads = num_heads # 8\n        self.head_dim = d_model // num_heads # 64\n        self.qkv_layer = nn.Linear(d_model, 3 * d_model) # 512 -> 1536\n        self.linear_layer = nn.Linear(d_model, d_model) # 512 -> 512\n        \n        \n    def forward(self,x,mask=None):\n        batch_size, seq_len, d_model = x.shape # 30 x 200 x 512\n        qkv = self.qkv_layer(x) # 30 x 200 x 1536\n        qkv = qkv.reshape(batch_size,seq_len,self.num_heads, 3*self.head_dim) # 30 x 200 x 8 x 192\n        qkv = qkv.permute(0,2,1,3) # 30 x 8 x 200 x 192\n        q, k, v = qkv.chunk(3,dim=-1) # [30 x 8 x 200 x 64] q,k,v\n        values, attentions = scaled_dot_product(q, k, v, mask=None) # values = 30 x 8 x 200 x 64 , attention = 30 x 8 x 200 x 200 \n        values = values.reshape(batch_size,seq_len,self.num_heads*self.head_dim) # 30 x 200 x 512\n        out = self.linear_layer(values) # 30 x 200 x 512\n        return out\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:21:24.430852Z","iopub.execute_input":"2024-06-13T16:21:24.431647Z","iopub.status.idle":"2024-06-13T16:21:24.452617Z","shell.execute_reply.started":"2024-06-13T16:21:24.431603Z","shell.execute_reply":"2024-06-13T16:21:24.451027Z"},"trusted":true},"execution_count":251,"outputs":[]},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, hidden, drop_prob=0.1):\n        super(PositionwiseFeedForward,self).__init__()\n        self.linear1 = nn.Linear(d_model,hidden) # 512 x hidden\n        self.linear2 = nn.Linear(hidden,d_model) # hidden x 512\n        self.relu = nn.ReLU() \n        self.dropout = nn.Dropout(p=drop_prob)\n        \n    def forward(self,x): # 30 x 200 x 512\n        x = self.linear1(x) # 30 x 512 x hidden\n        x = self.relu(x) # 30 x 512 x hidden\n        x = self.dropout(x) # 30 x 200 x hidden\n        x = self.linear2(x) # 30 x 200 x 512\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:21:24.454768Z","iopub.execute_input":"2024-06-13T16:21:24.455214Z","iopub.status.idle":"2024-06-13T16:21:24.469948Z","shell.execute_reply.started":"2024-06-13T16:21:24.455172Z","shell.execute_reply":"2024-06-13T16:21:24.468658Z"},"trusted":true},"execution_count":252,"outputs":[]},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n    def __init__(self, parameters_shape, eps=1e-5):\n        super().__init__()\n        self.parameters_shape = parameters_shape # [512]\n        self.eps = eps \n        self.gamma = nn.Parameter(torch.ones(parameters_shape)) # 512\n        self.beta = nn.Parameter(torch.zeros(parameters_shape)) # 512\n        \n    def forward(self, inputs): # inputs = 30 x 200 x 512\n        dims = [-(i+1) for i in range(len(self.parameters_shape))] # [-1]\n        mean = inputs.mean(dim=dims, keepdim = True) # 30 x 200 x 1\n        var = ((inputs-mean)**2).mean(dim=dims, keepdim=True) # 30 x 200 x 1\n        std = (var + self.eps).sqrt() # 30 x 200 x 1\n        y = (inputs-mean) / std # 30 x 200 x 512\n        out = self.gamma * y + self.beta # 30 x 200 x 512\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:21:24.471406Z","iopub.execute_input":"2024-06-13T16:21:24.472206Z","iopub.status.idle":"2024-06-13T16:21:24.489843Z","shell.execute_reply.started":"2024-06-13T16:21:24.472164Z","shell.execute_reply":"2024-06-13T16:21:24.488460Z"},"trusted":true},"execution_count":253,"outputs":[]},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n        super(EncoderLayer,self).__init__()\n        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout1 = nn.Dropout(p=drop_prob)\n        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout2 = nn.Dropout(p=drop_prob)\n    \n    def forward(self, x):\n        residual_x = x # 30 x 200 x 512\n        x = self.attention(x, mask=None) # 30 x 200 x 512\n        x = self.dropout1(x) # 30 x 200 x 512\n        x = self.norm1(x + residual_x) # 30 x 200 x 512 \n        \n        residual_x = x # 30 x 200 x 512\n        \n        x = self.ffn(x) # 30 x 200 x 512\n        x = self.dropout2(x) # 30 x 200 x 512\n        x = self.norm2(x + residual_x) # 30 x 200 x 512\n        \n        return x # 30 x 200 x 512","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:21:24.491656Z","iopub.execute_input":"2024-06-13T16:21:24.492559Z","iopub.status.idle":"2024-06-13T16:21:24.504319Z","shell.execute_reply.started":"2024-06-13T16:21:24.492524Z","shell.execute_reply":"2024-06-13T16:21:24.502992Z"},"trusted":true},"execution_count":254,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self,d_model,ffn_hidden,num_head,drop_prob,num_layers):\n        super().__init__()\n        self.layers = nn.Sequential(*[EncoderLayer(d_model,ffn_hidden,num_heads,\n                                                  drop_prob) for _ in range(num_layers)])\n        # input is passed through 5 or 10 number of EncoderLayers in\n        # sequential manner.\n    \n    def forward(self,x):\n        x = self.layers(x) \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:21:24.506496Z","iopub.execute_input":"2024-06-13T16:21:24.506943Z","iopub.status.idle":"2024-06-13T16:21:24.524990Z","shell.execute_reply.started":"2024-06-13T16:21:24.506905Z","shell.execute_reply":"2024-06-13T16:21:24.523560Z"},"trusted":true},"execution_count":255,"outputs":[]},{"cell_type":"code","source":"class MultiHeadCrossAttention(nn.Module):\n    def __init__(self,d_model,num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.kv_layer = nn.Linear(d_model, 2 * d_model)\n        self.q_layer = nn.Linear(d_model, d_model)\n        self.linear_layer = nn.Linear(d_model, d_model)\n        \n    def forward(self, x, y, mask=None):\n        batch_size, seq_len, d_model = x.shape # 30 x 200 x 512\n        kv = self.kv_layer(x) # 30 x 200 x 1536\n        q = self.q_layer(y)\n        kv = kv.reshape(batch_size, seq_len, self.num_heads, 2 * self.head_dim) # 30 x 200 x 8 x 192\n        q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n        \n        kv = kv.permute(0,2,1,3) # 30 x 8 x 200 x 192\n        q = q.permute(0,2,1,3)\n        \n        k, v = kv.chunk(2,dim=-1) # [30 x 8 x 200 x 64] k,v\n\n        values, attentions = scaled_dot_product(q, k, v, mask) # values = 30 x 8 x 200 x 64 , attention = 30 x 8 x 200 x 200 \n        values = values.reshape(batch_size,seq_len,self.num_heads*self.head_dim) # 30 x 200 x 512\n        out = self.linear_layer(values) # 30 x 200 x 512\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:21:24.529297Z","iopub.execute_input":"2024-06-13T16:21:24.529791Z","iopub.status.idle":"2024-06-13T16:21:24.542082Z","shell.execute_reply.started":"2024-06-13T16:21:24.529749Z","shell.execute_reply":"2024-06-13T16:21:24.540752Z"},"trusted":true},"execution_count":256,"outputs":[]},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n        super(DecoderLayer,self).__init__()\n        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.norm1 = LayerNormalization([d_model])\n        self.dropout1 = nn.Dropout(p=drop_prob)\n        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout2 = nn.Dropout(p=drop_prob)\n        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.norm3 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout3 = nn.Dropout(p=drop_prob)\n    \n    def forward(self, x, y, decoder_mask):\n        _y = y\n        print(\"Masked Self Attention\")\n        y = self.self_attention(x,mask=decoder_mask)\n        print(\"Dropout 1\")\n        y = self.dropout1(y)\n        print(\"Add + Layer Nomralization 1\")\n        y = self.norm1(y + _y)\n        \n        _y = y\n        print(\"Cross Attention\")\n        y = self.encoder_decoder_attention(x, y, mask=None)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:21:24.543925Z","iopub.execute_input":"2024-06-13T16:21:24.544303Z","iopub.status.idle":"2024-06-13T16:21:24.564597Z","shell.execute_reply.started":"2024-06-13T16:21:24.544263Z","shell.execute_reply":"2024-06-13T16:21:24.563274Z"},"trusted":true},"execution_count":257,"outputs":[]},{"cell_type":"code","source":"class SequentialDecoder(nn.Sequential):\n    def forward(self,*inputs):\n        x, y, mask = inputs\n        for module in self._modules.values():\n            y = module(x, y, mask)\n        return y","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:21:24.566049Z","iopub.execute_input":"2024-06-13T16:21:24.566396Z","iopub.status.idle":"2024-06-13T16:21:24.579630Z","shell.execute_reply.started":"2024-06-13T16:21:24.566367Z","shell.execute_reply":"2024-06-13T16:21:24.578363Z"},"trusted":true},"execution_count":258,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers=1):\n        super().__init__()\n        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n        \n    def forward(self,x,y,mask):\n        # x : 30 x 200 x 512\n        # y : 30 x 200 x 512\n        # mask : 200 x 200\n        y = self.layers(x,y,mask)\n        return y","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:21:24.581139Z","iopub.execute_input":"2024-06-13T16:21:24.581483Z","iopub.status.idle":"2024-06-13T16:21:24.594333Z","shell.execute_reply.started":"2024-06-13T16:21:24.581455Z","shell.execute_reply":"2024-06-13T16:21:24.593031Z"},"trusted":true},"execution_count":259,"outputs":[]},{"cell_type":"code","source":"d_model = 512\nbatch_size = 30\nnum_heads = 8\ndrop_prob = 0.1\nmax_seq_len = 200\nffn_hidden = 2048\nnum_layers = 5\nencoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:21:24.596653Z","iopub.execute_input":"2024-06-13T16:21:24.597193Z","iopub.status.idle":"2024-06-13T16:21:24.753860Z","shell.execute_reply.started":"2024-06-13T16:21:24.597152Z","shell.execute_reply":"2024-06-13T16:21:24.752708Z"},"trusted":true},"execution_count":260,"outputs":[]},{"cell_type":"code","source":"x = torch.randn((30,200,512))\nout = encoder(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:21:24.755397Z","iopub.execute_input":"2024-06-13T16:21:24.755836Z","iopub.status.idle":"2024-06-13T16:21:28.401226Z","shell.execute_reply.started":"2024-06-13T16:21:24.755795Z","shell.execute_reply":"2024-06-13T16:21:28.399778Z"},"trusted":true},"execution_count":261,"outputs":[]},{"cell_type":"code","source":"out.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:21:28.404540Z","iopub.execute_input":"2024-06-13T16:21:28.404975Z","iopub.status.idle":"2024-06-13T16:21:28.413588Z","shell.execute_reply.started":"2024-06-13T16:21:28.404944Z","shell.execute_reply":"2024-06-13T16:21:28.412190Z"},"trusted":true},"execution_count":262,"outputs":[{"execution_count":262,"output_type":"execute_result","data":{"text/plain":"torch.Size([30, 200, 512])"},"metadata":{}}]},{"cell_type":"code","source":"x = out\ny = torch.randn((batch_size,max_seq_len,d_model))\nmask = torch.full([max_seq_len, max_seq_len] , float('-inf'))\nmask = torch.triu(mask, diagonal =1)\ndecoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\nout = decoder(x, y, mask)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:21:28.415119Z","iopub.execute_input":"2024-06-13T16:21:28.415457Z","iopub.status.idle":"2024-06-13T16:21:29.619573Z","shell.execute_reply.started":"2024-06-13T16:21:28.415428Z","shell.execute_reply":"2024-06-13T16:21:29.617898Z"},"trusted":true},"execution_count":263,"outputs":[{"name":"stdout","text":"Masked Self Attention\nDropout 1\nAdd + Layer Nomralization 1\nCross Attention\nMasked Self Attention\nDropout 1\nAdd + Layer Nomralization 1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[263], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtriu(mask, diagonal \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n\u001b[0;32m----> 6\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[259], line 10\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, y, mask)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,y,mask):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# x : 30 x 200 x 512\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# y : 30 x 200 x 512\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# mask : 200 x 200\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[258], line 5\u001b[0m, in \u001b[0;36mSequentialDecoder.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m      3\u001b[0m x, y, mask \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m----> 5\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[257], line 21\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x, y, decoder_mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(y)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdd + Layer Nomralization 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_y\u001b[49m)\n\u001b[1;32m     23\u001b[0m _y \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross Attention\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'Tensor' and 'NoneType'"],"ename":"TypeError","evalue":"unsupported operand type(s) for +: 'Tensor' and 'NoneType'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}