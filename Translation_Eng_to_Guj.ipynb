{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2119948,"sourceType":"datasetVersion","datasetId":1272055}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-06T04:49:47.243375Z","iopub.execute_input":"2024-07-06T04:49:47.243788Z","iopub.status.idle":"2024-07-06T04:49:47.267852Z","shell.execute_reply.started":"2024-07-06T04:49:47.243754Z","shell.execute_reply":"2024-07-06T04:49:47.266911Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"/kaggle/input/samanantar/final_data/en-ta/train.ta\n/kaggle/input/samanantar/final_data/en-ta/train.en\n/kaggle/input/samanantar/final_data/en-ml/train.ml\n/kaggle/input/samanantar/final_data/en-ml/train.en\n/kaggle/input/samanantar/final_data/en-as/train.as\n/kaggle/input/samanantar/final_data/en-as/train.en\n/kaggle/input/samanantar/final_data/en-kn/train.kn\n/kaggle/input/samanantar/final_data/en-kn/train.en\n/kaggle/input/samanantar/final_data/en-pa/train.pa\n/kaggle/input/samanantar/final_data/en-pa/train.en\n/kaggle/input/samanantar/final_data/en-mr/train.mr\n/kaggle/input/samanantar/final_data/en-mr/train.en\n/kaggle/input/samanantar/final_data/en-hi/train.hi\n/kaggle/input/samanantar/final_data/en-hi/train.en\n/kaggle/input/samanantar/final_data/en-bn/train.bn\n/kaggle/input/samanantar/final_data/en-bn/train.en\n/kaggle/input/samanantar/final_data/en-te/train.te\n/kaggle/input/samanantar/final_data/en-te/train.en\n/kaggle/input/samanantar/final_data/en-or/train.or\n/kaggle/input/samanantar/final_data/en-or/train.en\n/kaggle/input/samanantar/final_data/en-gu/train.gu\n/kaggle/input/samanantar/final_data/en-gu/train.en\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\nimport numpy as np\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:49:47.269665Z","iopub.execute_input":"2024-07-06T04:49:47.269960Z","iopub.status.idle":"2024-07-06T04:49:47.274576Z","shell.execute_reply.started":"2024-07-06T04:49:47.269936Z","shell.execute_reply":"2024-07-06T04:49:47.273576Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# import os\n# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:49:47.275742Z","iopub.execute_input":"2024-07-06T04:49:47.276064Z","iopub.status.idle":"2024-07-06T04:49:47.284284Z","shell.execute_reply.started":"2024-07-06T04:49:47.276034Z","shell.execute_reply":"2024-07-06T04:49:47.283462Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n# device = torch.device('cpu')\nprint(f\"Using device: {device}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:49:47.285492Z","iopub.execute_input":"2024-07-06T04:49:47.285827Z","iopub.status.idle":"2024-07-06T04:49:47.295490Z","shell.execute_reply.started":"2024-07-06T04:49:47.285796Z","shell.execute_reply":"2024-07-06T04:49:47.294547Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# English_file = \"/kaggle/input/samanantar/final_data/en-gu/train.en\"\n# Gujarati_file = \"/kaggle/input/samanantar/final_data/en-gu/train.gu\"\n\n\nEnglish_file = \"/kaggle/input/samanantar/final_data/en-gu/train.en\"\nGujarati_file = \"/kaggle/input/samanantar/final_data/en-gu/train.gu\"","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:49:47.297457Z","iopub.execute_input":"2024-07-06T04:49:47.297765Z","iopub.status.idle":"2024-07-06T04:49:47.306045Z","shell.execute_reply.started":"2024-07-06T04:49:47.297732Z","shell.execute_reply":"2024-07-06T04:49:47.305142Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"START_TOKEN = '<START>'\nPADDING_TOKEN = '<PADDING>'\nEND_TOKEN = '<END>'","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:49:47.307070Z","iopub.execute_input":"2024-07-06T04:49:47.307331Z","iopub.status.idle":"2024-07-06T04:49:47.317062Z","shell.execute_reply.started":"2024-07-06T04:49:47.307308Z","shell.execute_reply":"2024-07-06T04:49:47.316218Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Gujarati_vocab = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n#                       '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ˌ', \n#                       'ँ', 'ఆ', 'ఇ', 'ా', 'ి', 'ీ', 'ు', 'ూ', \n#                       'ಅ', 'ಆ', 'ಇ', 'ಈ', 'ಉ', 'ಊ', 'ಋ', 'ೠ', 'ಌ', 'ಎ', 'ಏ', 'ಐ', 'ಒ', 'ಓ', 'ಔ', \n#                       'ಕ', 'ಖ', 'ಗ', 'ಘ', 'ಙ', \n#                       'ಚ', 'ಛ', 'ಜ', 'ಝ', 'ಞ', \n#                       'ಟ', 'ಠ', 'ಡ', 'ಢ', 'ಣ', \n#                       'ತ', 'ಥ', 'ದ', 'ಧ', 'ನ', \n#                       'ಪ', 'ಫ', 'ಬ', 'ಭ', 'ಮ', \n#                       'ಯ', 'ರ', 'ಱ', 'ಲ', 'ಳ', 'ವ', 'ಶ', 'ಷ', 'ಸ', 'ಹ', \n#                       '಼', 'ಽ', 'ಾ', 'ಿ', 'ೀ', 'ು', 'ೂ', 'ೃ', 'ೄ', 'ೆ', 'ೇ', 'ೈ', 'ೊ', 'ೋ', 'ೌ', '್', 'ೕ', 'ೖ', 'ೞ', 'ೣ', 'ಂ', 'ಃ', \n#                       '೦', '೧', '೨', '೩', '೪', '೫', '೬', '೭', '೮', '೯', PADDING_TOKEN, END_TOKEN]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:49:47.318266Z","iopub.execute_input":"2024-07-06T04:49:47.318687Z","iopub.status.idle":"2024-07-06T04:49:47.328547Z","shell.execute_reply.started":"2024-07-06T04:49:47.318657Z","shell.execute_reply":"2024-07-06T04:49:47.327824Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"\n# Gujarati letters and symbols\ngujarati_letters = [\n    'અ', 'આ', 'ઇ', 'ઈ', 'ઉ', 'ઊ', 'ઋ', 'ઌ', 'એ', 'ઐ', 'ઓ', 'ઔ', 'ક', 'ખ', 'ગ', 'ઘ', 'ઙ', 'ચ', 'છ', 'જ', 'ઝ', 'ઞ', 'ટ', 'ઠ', 'ડ', 'ઢ', 'ણ', 'ત', 'થ', 'દ', 'ધ', 'ન',\n    'પ', 'ફ', 'બ', 'ભ', 'મ', 'ય', 'ર', 'લ', 'ળ', 'વ', 'શ', 'ષ', 'સ', 'હ', 'ા', 'િ', 'ી', 'ુ', 'ૂ', 'ૃ', 'ૄ', 'ૅ', 'ે', 'ૈ', 'ો', 'ૌ', 'ૐ', 'ૡ', 'ૢ', '૱',\n    'ં', 'ઃ', 'ૠ', 'ૉ', '૰', '્', 'ઁ', 'ઍ', '઼', 'ઑ'\n]\n\n# Gujarati numerals\ngujarati_numerals = [chr(code) for code in range(0x0AE6, 0x0AEF + 1)]\nprint(gujarati_numerals)\n# Additional symbols and characters\n# additional_symbols = [\n#     '”', '“', '‘', '~', ']', '\\\\', '»', '@', '❑', '`', '}', '|', '–', '_', '’', '।', '्', '©', '½', '[', '°', '₹', '{', '…'\n# ]\n\n# Common symbols and punctuation used in Gujarati text\nother_symbols = [\n    ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n\n    ':', '<', '=', '>', '?'\n]\n\n# Combine all characters into one vocabulary list\nGujarati_vocab = [START_TOKEN] + gujarati_letters + gujarati_numerals  + other_symbols + [PADDING_TOKEN, END_TOKEN]\n\n# Example usage\nprint(\"Gujarati Vocabulary:\")\nprint(Gujarati_vocab)\nprint(len(Gujarati_vocab))\nprint(len(set(Gujarati_vocab)))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:49:47.329651Z","iopub.execute_input":"2024-07-06T04:49:47.329929Z","iopub.status.idle":"2024-07-06T04:49:47.344080Z","shell.execute_reply.started":"2024-07-06T04:49:47.329907Z","shell.execute_reply":"2024-07-06T04:49:47.343117Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"['૦', '૧', '૨', '૩', '૪', '૫', '૬', '૭', '૮', '૯']\nGujarati Vocabulary:\n['<START>', 'અ', 'આ', 'ઇ', 'ઈ', 'ઉ', 'ઊ', 'ઋ', 'ઌ', 'એ', 'ઐ', 'ઓ', 'ઔ', 'ક', 'ખ', 'ગ', 'ઘ', 'ઙ', 'ચ', 'છ', 'જ', 'ઝ', 'ઞ', 'ટ', 'ઠ', 'ડ', 'ઢ', 'ણ', 'ત', 'થ', 'દ', 'ધ', 'ન', 'પ', 'ફ', 'બ', 'ભ', 'મ', 'ય', 'ર', 'લ', 'ળ', 'વ', 'શ', 'ષ', 'સ', 'હ', 'ા', 'િ', 'ી', 'ુ', 'ૂ', 'ૃ', 'ૄ', 'ૅ', 'ે', 'ૈ', 'ો', 'ૌ', 'ૐ', 'ૡ', 'ૢ', '૱', 'ં', 'ઃ', 'ૠ', 'ૉ', '૰', '્', 'ઁ', 'ઍ', '઼', 'ઑ', '૦', '૧', '૨', '૩', '૪', '૫', '૬', '૭', '૮', '૯', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', '<PADDING>', '<END>']\n116\n116\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# d = set()\n\n# for i in Gujarati_vocab:\n#     if i not in d:\n#         d.add(i)\n#     else:\n#         print(i)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:49:47.361468Z","iopub.execute_input":"2024-07-06T04:49:47.362330Z","iopub.status.idle":"2024-07-06T04:49:47.365951Z","shell.execute_reply.started":"2024-07-06T04:49:47.362292Z","shell.execute_reply":"2024-07-06T04:49:47.365062Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# English_vocab = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n#                         '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n#                         ':', '<', '=', '>', '?', '@',\n#                         '[', chr(92), \"]\", \"^\", \"_\", \"`\", \n#                         'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n#                         'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n#                         'y', 'z','S',\n#                         '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]\nEnglish_vocab = [\n    START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n    ':', '<', '=', '>', '?', '@',\n    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n    'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n    'Y', 'Z',\n    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n    'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n    'y', 'z',\n    PADDING_TOKEN, END_TOKEN\n] \n \n# Print the length of English vocabulary\nprint(\"English vocabulary:\", (English_vocab))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:49:47.367496Z","iopub.execute_input":"2024-07-06T04:49:47.367952Z","iopub.status.idle":"2024-07-06T04:49:47.377272Z","shell.execute_reply.started":"2024-07-06T04:49:47.367921Z","shell.execute_reply":"2024-07-06T04:49:47.376267Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"English vocabulary: ['<START>', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '<PADDING>', '<END>']\n","output_type":"stream"}]},{"cell_type":"code","source":"index_to_gujarati = {k:v for k,v in enumerate(Gujarati_vocab)}\ngujarati_to_index = {v:k for k,v in enumerate(Gujarati_vocab)}\nindex_to_english = {k:v for k,v in enumerate(English_vocab)}\nenglish_to_index = {v:k for k,v in enumerate(English_vocab)}","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:49:47.378497Z","iopub.execute_input":"2024-07-06T04:49:47.378898Z","iopub.status.idle":"2024-07-06T04:49:47.390589Z","shell.execute_reply.started":"2024-07-06T04:49:47.378866Z","shell.execute_reply":"2024-07-06T04:49:47.389840Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"with open(Gujarati_file, 'r') as file:\n    gujarati_sentences = file.readlines()\nwith open(English_file, 'r' ) as file:\n    english_sentences = file.readlines()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:49:47.392571Z","iopub.execute_input":"2024-07-06T04:49:47.392834Z","iopub.status.idle":"2024-07-06T04:49:50.313845Z","shell.execute_reply.started":"2024-07-06T04:49:47.392812Z","shell.execute_reply":"2024-07-06T04:49:50.312961Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"print(len(gujarati_sentences))\nprint(len(english_sentences))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:49:50.314973Z","iopub.execute_input":"2024-07-06T04:49:50.315287Z","iopub.status.idle":"2024-07-06T04:49:50.320295Z","shell.execute_reply.started":"2024-07-06T04:49:50.315260Z","shell.execute_reply":"2024-07-06T04:49:50.319385Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"3054273\n3054273\n","output_type":"stream"}]},{"cell_type":"code","source":"# import sentencepiece as spm\n\n\n# # Train SentencePiece model with a target vocabulary size\n# vocab_size = 20000 # You can adjust this size as needed\n# # \n# spm.SentencePieceTrainer.train(input='/kaggle/input/samanantar/final_data/en-gu/train.gu', model_prefix='gujarati', vocab_size=vocab_size, character_coverage=1.0, model_type='bpe')\n\n# # Load the trained model\n# sp = spm.SentencePieceProcessor()\n# sp.load('gujarati.model')\n\n# # Test the tokenizer\n# test_sentence = \"આ એક ઉદાહરણ છે.\"\n# tokens = sp.encode_as_pieces(test_sentence)\n# token_ids = sp.encode_as_ids(test_sentence)\n# print(sp.vocab_size)\n# print(\"Tokens:\", tokens)\n# print(\"Token IDs:\", token_ids)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:49:50.321289Z","iopub.execute_input":"2024-07-06T04:49:50.321552Z","iopub.status.idle":"2024-07-06T04:49:50.331082Z","shell.execute_reply.started":"2024-07-06T04:49:50.321530Z","shell.execute_reply":"2024-07-06T04:49:50.330206Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i,j  in zip(gujarati_sentences[150:160],english_sentences[150:160]):\n    print(j,i)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:49:50.332218Z","iopub.execute_input":"2024-07-06T04:49:50.332611Z","iopub.status.idle":"2024-07-06T04:49:50.344881Z","shell.execute_reply.started":"2024-07-06T04:49:50.332581Z","shell.execute_reply":"2024-07-06T04:49:50.343910Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"As for the one listening to me, he will reside in security and be undisturbed from dread of calamity.\n યહોવાહ કહે છે કે “મૂર્ખોની બેદરકારી તેઓનો વિનાશ કરશે.\n\nThe entire power is with you.\n સર્વ શક્તિ તમારી અંદર છે.\n\nShah Rukh Khan was conferred an honorary doctorate by the La Trobe University, Melbourne, for his work supporting women's empowerment and his achievement in the Indian film industry.\n મહિલાઓ અને જરૂરિયાતવાળા બાળકોનાં ઉજ્જવળ ભવિષ્ય માટે સાથે જ ભારતીય સિનેમામાં આપેલા યોગદાન માટે શાહરુખ ખાનને ઑસ્ટ્રેલિયાનાં મેલબર્નમાં આવેલી લા ટ્રોબ યુનિવર્સિટીએ સન્માનનિય ડિગ્રી એવી ‘ડૉક્ટર ઑફ લેટર્સ’થી સન્માનિત કરવામાં આવ્યો છે.\n\nA group of people standing around a green train car.\n ગ્રીન ટ્રેન કારની આસપાસ ઊભેલા લોકોનો સમૂહ\n\nAnd then it started raining.\n અને પછી શરૂ થયો ધોધમાર વરસાદ !\n\n4: 12, 13. Why can Christians rejoice when they are persecuted?\n આપણો વિરોધ કે સતાવણી થાય તો શા માટે હરખાવું જોઈએ?\n\nHe had many ups and downs in his life.\n તેમના જીવન માં, ત્યાં ઘણા અપ્સ એન્ડ ડાઉન્સ હતી.\n\nWe have no such confusion.\n આપણે ત્યાં આવી સંકુચિતતા નથી.\n\nStay calm... it will happen only when we want it.\n શાંતિ રાખો આ ત્યારે જ થશે જ્યારે અમારી ઈચ્છા હશે.\n\nwhat are trps?\n ટેરિયર્સ શું છે?\n\n","output_type":"stream"}]},{"cell_type":"code","source":"total_sentences = 200000\nenglish_sntcs = english_sentences[:total_sentences]\ngujarati_sntcs = gujarati_sentences[:total_sentences]\nenglish_sntcs = [sentence.rstrip('\\n') for sentence in english_sntcs]\ngujarati_sntcs = [sentence.rstrip('\\n') for sentence in gujarati_sntcs]","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:49:50.346051Z","iopub.execute_input":"2024-07-06T04:49:50.346367Z","iopub.status.idle":"2024-07-06T04:49:50.460170Z","shell.execute_reply.started":"2024-07-06T04:49:50.346339Z","shell.execute_reply":"2024-07-06T04:49:50.459357Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"max_seq_len = 200\n\ndef is_valid_tokens(sentence, vocab):\n    for token in list(set(sentence)):\n        if token not in vocab:\n            return False\n    return True\n\ndef is_valid_length(sentence, max_sequence_length):\n    return len(list(sentence)) < (max_sequence_length - 1) # need to add <START> and <END> tokens\n\nvalid_sntcs_idx = []\nfor index in range(len(gujarati_sntcs)):\n    guj_sntc , eng_sntc = gujarati_sntcs[index], english_sntcs[index]\n    if is_valid_length(guj_sntc, max_seq_len) \\\n        and is_valid_length(eng_sntc, max_seq_len) \\\n        and is_valid_tokens(guj_sntc, Gujarati_vocab)\\\n        and is_valid_tokens(eng_sntc, English_vocab):\n        \n        valid_sntcs_idx.append(index)\n        \nprint(len(valid_sntcs_idx))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:49:50.461309Z","iopub.execute_input":"2024-07-06T04:49:50.461660Z","iopub.status.idle":"2024-07-06T04:50:03.362901Z","shell.execute_reply.started":"2024-07-06T04:49:50.461630Z","shell.execute_reply":"2024-07-06T04:50:03.361931Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"178369\n","output_type":"stream"}]},{"cell_type":"code","source":"english_sntcs = [english_sntcs[i] for i in valid_sntcs_idx]\ngujarati_sntcs = [gujarati_sntcs[i] for i in valid_sntcs_idx]","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:50:03.367291Z","iopub.execute_input":"2024-07-06T04:50:03.367601Z","iopub.status.idle":"2024-07-06T04:50:03.407077Z","shell.execute_reply.started":"2024-07-06T04:50:03.367576Z","shell.execute_reply":"2024-07-06T04:50:03.406044Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass TextDataset(Dataset):\n    \n    def __init__(self, english_sntcs, gujarati_sntcs):\n        self.english_sntcs = english_sntcs\n        self.gujarati_sntcs = gujarati_sntcs\n        \n    def __len__(self):\n        return len(self.english_sntcs)\n    \n    def __getitem__(self,idx):\n        return self.english_sntcs[idx], self.gujarati_sntcs[idx]","metadata":{"execution":{"iopub.status.busy":"2024-07-06T04:50:03.408363Z","iopub.execute_input":"2024-07-06T04:50:03.408752Z","iopub.status.idle":"2024-07-06T04:50:03.418011Z","shell.execute_reply.started":"2024-07-06T04:50:03.408718Z","shell.execute_reply":"2024-07-06T04:50:03.417219Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"dataset = TextDataset(english_sntcs, gujarati_sntcs)\nlen(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:58:10.873893Z","iopub.execute_input":"2024-07-06T05:58:10.874799Z","iopub.status.idle":"2024-07-06T05:58:10.880530Z","shell.execute_reply.started":"2024-07-06T05:58:10.874764Z","shell.execute_reply":"2024-07-06T05:58:10.879620Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"178369"},"metadata":{}}]},{"cell_type":"code","source":"batch_size = 32\n\ntrain_loader = DataLoader(dataset, batch_size)\nlen(train_loader)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:58:12.795822Z","iopub.execute_input":"2024-07-06T05:58:12.796500Z","iopub.status.idle":"2024-07-06T05:58:12.802372Z","shell.execute_reply.started":"2024-07-06T05:58:12.796468Z","shell.execute_reply":"2024-07-06T05:58:12.801455Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"5575"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\nd_model = 512\nffn_hidden = 2048\nnum_heads = 8\ndrop_prob = 0.1\nnum_layers = 2\nmax_sequence_length = max_seq_len = 200\nguj_vocab_size = len(Gujarati_vocab)\n\ntransformer = Transformer(d_model, \n                          ffn_hidden,\n                          num_heads, \n                          drop_prob, \n                          num_layers, \n                          max_seq_len,\n                          guj_vocab_size,\n                          english_to_index,\n                          gujarati_to_index,\n                          START_TOKEN, \n                          END_TOKEN, \n                          PADDING_TOKEN)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:58:23.179333Z","iopub.execute_input":"2024-07-06T05:58:23.179762Z","iopub.status.idle":"2024-07-06T05:58:23.315752Z","shell.execute_reply.started":"2024-07-06T05:58:23.179733Z","shell.execute_reply":"2024-07-06T05:58:23.314674Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\ncriterian = nn.CrossEntropyLoss(ignore_index=gujarati_to_index[PADDING_TOKEN],\n                                reduction='none')\n\n# When computing the loss, we are ignoring cases when the label is the padding token\nfor params in transformer.parameters():\n    if params.dim() > 1:\n        nn.init.xavier_uniform_(params)\n\noptim = torch.optim.Adam(transformer.parameters(), lr=0.0001)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:58:25.916564Z","iopub.execute_input":"2024-07-06T05:58:25.917411Z","iopub.status.idle":"2024-07-06T05:58:26.041755Z","shell.execute_reply.started":"2024-07-06T05:58:25.917364Z","shell.execute_reply":"2024-07-06T05:58:26.040758Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"NEG_INFTY = -1e9\n\ndef create_masks(eng_batch, kn_batch):\n    num_sentences = len(eng_batch)\n    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n\n    for idx in range(num_sentences):\n        eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n        eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n        kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)\n        encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n        encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n        decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n        decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n        decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n        decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n\n    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:58:26.233305Z","iopub.execute_input":"2024-07-06T05:58:26.233965Z","iopub.status.idle":"2024-07-06T05:58:26.243641Z","shell.execute_reply.started":"2024-07-06T05:58:26.233935Z","shell.execute_reply":"2024-07-06T05:58:26.242632Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\n\nNEG_INFTY = -1e9\n\ndef create_masks1(eng_batch, guj_batch):\n    num_sentences = len(eng_batch)\n    look_ahead_mask = torch.full([max_seq_len, max_seq_len] , True)\n    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n    encoder_padding_mask = torch.full([num_sentences, max_seq_len, max_seq_len] , False)\n    decoder_padding_mask_self_attention = torch.full([num_sentences, max_seq_len, max_seq_len] , False)\n    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_seq_len, max_seq_len] , False)\n    \n    for idx in range(num_sentences):\n        eng_sentence_length, guj_sentence_length = len(eng_batch[idx]), len(guj_batch[idx])\n#         print(eng_sentence_length,guj_sentence_length)\n        eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_seq_len)\n        guj_chars_to_padding_mask = np.arange(guj_sentence_length + 1, max_seq_len)\n        encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n        encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n        decoder_padding_mask_self_attention[idx, :, guj_chars_to_padding_mask] = True\n        decoder_padding_mask_self_attention[idx, guj_chars_to_padding_mask, :] = True\n        decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n        decoder_padding_mask_cross_attention[idx, guj_chars_to_padding_mask, :] = True\n\n    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n    print(f\"encoder_self_attention_mask {encoder_self_attention_mask.size()}: {encoder_self_attention_mask[0, :50, :50]}\")\n    print(f\"decoder_self_attention_mask {decoder_self_attention_mask.size()}: {decoder_self_attention_mask[0, :50, :50]}\")\n    print(f\"decoder_cross_attention_mask {decoder_cross_attention_mask.size()}: {decoder_cross_attention_mask[0, :50, :50]}\")\n    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n        \n# Corrected function call with provided sentences\nguj_sentence = (\"gi\",)\neng_sentence = (\"should we go to the mall?\",)\nencoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks1(eng_sentence, guj_sentence)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:58:26.541031Z","iopub.execute_input":"2024-07-06T05:58:26.541661Z","iopub.status.idle":"2024-07-06T05:58:26.558695Z","shell.execute_reply.started":"2024-07-06T05:58:26.541629Z","shell.execute_reply":"2024-07-06T05:58:26.557796Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"encoder_self_attention_mask torch.Size([1, 200, 200]): tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        ...,\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09]])\ndecoder_self_attention_mask torch.Size([1, 200, 200]): tensor([[ 0.0000e+00, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [ 0.0000e+00,  0.0000e+00, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        ...,\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09]])\ndecoder_cross_attention_mask torch.Size([1, 200, 200]): tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        ...,\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09],\n        [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n         -1.0000e+09, -1.0000e+09]])\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ntransformer.train()\nprint(device)\ntransformer.to(device)\ntotal_loss = 0\nnum_epochs = 10\nprint(train_loader)\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch}\")\n    iterator = iter(train_loader)\n    print(len(train_loader))\n    for batch_num, batch in enumerate(iterator):\n        transformer.train()\n        eng_batch, guj_batch = batch\n        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, guj_batch)\n#         print(encoder_self_attention_mask.shape,decoder_self_attention_mask.shape, decoder_cross_attention_mask.shape)\n#         print(encoder_self_attention_mask[0][0],encoder_self_attention_mask[1][0],encoder_self_attention_mask[0][2])\n        optim.zero_grad()\n        guj_predictions = transformer(eng_batch,\n                                     guj_batch,\n                                     encoder_self_attention_mask.to(device), \n                                     decoder_self_attention_mask.to(device), \n                                     decoder_cross_attention_mask.to(device),\n                                     enc_start_token=False,\n                                     enc_end_token=False,\n                                     dec_start_token=True,\n                                     dec_end_token=True)\n        labels = transformer.decoder.sentence_embedding.batch_tokenize(guj_batch, start_token=False, end_token=True)\n        loss = criterian(\n            guj_predictions.view(-1, guj_vocab_size).to(device),\n            labels.view(-1).to(device)\n        ).to(device)\n        valid_indicies = torch.where(labels.view(-1) == gujarati_to_index[PADDING_TOKEN], False, True)\n        loss = loss.sum() / valid_indicies.sum()\n        loss.backward()\n        optim.step()\n        #train_losses.append(loss.item())\n        if batch_num % 1000 == 0:\n            print(f\"Iteration {batch_num} : {loss.item()}\")\n            print(f\"English: {eng_batch[0]}\")\n            print(f\"gujarati Translation: {guj_batch[0]}\")\n#             print(\"Mask\",encoder_self_attention_mask[0,:50,:50])\n            guj_sentence_predicted = torch.argmax(guj_predictions[0], axis=1)\n            predicted_sentence = \"\"\n            for idx in guj_sentence_predicted:\n                if idx == gujarati_to_index[END_TOKEN]:\n                    break\n                predicted_sentence += index_to_gujarati[idx.item()]\n            print(f\"gujarati Prediction: {predicted_sentence}\")\n\n\n            transformer.eval()\n            guj_sentence = (\"\",)\n            eng_sentence = (\"should we go to the mall?\",)\n#             print(encoder_self_attention_mask.shape,decoder_self_attention_mask.shape, decoder_cross_attention_mask.shape)\n#             print(encoder_self_attention_mask[0][0],encoder_self_attention_mask[0][1],encoder_self_attention_mask[0][2])\n               \n            for word_counter in range(max_seq_len):\n                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, guj_sentence)\n                predictions = transformer(eng_sentence,\n                                          guj_sentence,\n                                          encoder_self_attention_mask.to(device), \n                                          decoder_self_attention_mask.to(device), \n                                          decoder_cross_attention_mask.to(device),\n                                          enc_start_token=False,\n                                          enc_end_token=False,\n                                          dec_start_token=True,\n                                          dec_end_token=False)\n                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n                next_token_index = torch.argmax(next_token_prob_distribution).item()\n                next_token = index_to_gujarati[next_token_index]\n                guj_sentence = (guj_sentence[0] + next_token, )\n                if next_token == END_TOKEN:\n                    break\n            \n            print(f\"Evaluation translation (should we go to the mall?) : {guj_sentence}\")\n            print(\"-------------------------------------------\")\n        \n     ","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:36:22.482678Z","iopub.execute_input":"2024-07-06T06:36:22.483040Z","iopub.status.idle":"2024-07-06T08:47:34.677702Z","shell.execute_reply.started":"2024-07-06T06:36:22.483012Z","shell.execute_reply":"2024-07-06T08:47:34.676811Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"cuda\n<torch.utils.data.dataloader.DataLoader object at 0x7ee02256fbb0>\nEpoch 0\n5575\nIteration 0 : 1.5744199752807617\nEnglish: But many have tried to supply missing details.\ngujarati Translation: ઘણા જાતજાતની માહિતી અને અફવા ફેલાવે છે.\ngujarati Prediction: પણા વોતેીનિી સાતિતી અને પનવા મરલાવવ છે.\nEvaluation translation (should we go to the mall?) : ('તમે કેવી રીતે કેવી રીતે કરવા માટે?<END>',)\n-------------------------------------------\nIteration 1000 : 1.552311658859253\nEnglish: Truly, life is a game.\ngujarati Translation: સાચી રીતે જોઈએ તો જીવન એક ક્રીડાસ્થાન છે.\ngujarati Prediction: તારા સીતે તીઈ .છો છ તન છક છરયિ. ..ટા ીછે.\nEvaluation translation (should we go to the mall?) : ('તેઓ કેવી રીતે કરવું જોઈએ?<END>',)\n-------------------------------------------\nIteration 2000 : 1.632406234741211\nEnglish: Bhandup West: Ramesh Gajanan Korgaonkar (Shiv Sena)\ngujarati Translation: ભાંડુપ (પશ્ચિમ) - રમેશ કોરગાંવકર (શિવસેના)\ngujarati Prediction: રારડનર અરા્ચિમ  અ રાનશ કરં્્ર ા  સરાવસેના \nEvaluation translation (should we go to the mall?) : ('તમે તમે કેવી રીતે કરવા માટે?<END>',)\n-------------------------------------------\nIteration 3000 : 1.472113847732544\nEnglish: were given.\ngujarati Translation: જ આપવામાં આવ્યા હતા.\ngujarati Prediction: જેકપવામાં આવીયા હતા.\nEvaluation translation (should we go to the mall?) : ('તમે તમે કરવા માટે કરવા માટે છે?<END>',)\n-------------------------------------------\nIteration 4000 : 1.484871506690979\nEnglish: Jesus Christ is the primary figure in the outworking of Gods purpose.\ngujarati Translation: યહોવાહનો હેતુ સિદ્ધ કરવામાં ઈસુ ખ્રિસ્ત મુખ્ય ભૂમિકા ભજવે છે.\ngujarati Prediction: આહોવાહના સાલુ ક્ં્ધાપર્ાનાં આનુનકારિસ્ત કાલ્ય છાતિકા કાવી છે.\nEvaluation translation (should we go to the mall?) : ('તમે કોઈ કામ કરવા માટે?<END>',)\n-------------------------------------------\nIteration 5000 : 1.5836095809936523\nEnglish: 300 medical teachers resign\ngujarati Translation: 300 ડોક્ટર્સે આપ્યા રાજીનામા\ngujarati Prediction: 500 કિલ્ટરોસ  મવવયુ\nEvaluation translation (should we go to the mall?) : ('તમે તમે તમે કોઈ જાય છે?<END>',)\n-------------------------------------------\nEpoch 1\n5575\nIteration 0 : 1.4988030195236206\nEnglish: But many have tried to supply missing details.\ngujarati Translation: ઘણા જાતજાતની માહિતી અને અફવા ફેલાવે છે.\ngujarati Prediction: પરા લ ણેન ના સાટિતી અને પનસાયમરલાવવ છે.\nEvaluation translation (should we go to the mall?) : ('તમે કોઈ રીતે કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 1000 : 1.467625617980957\nEnglish: Truly, life is a game.\ngujarati Translation: સાચી રીતે જોઈએ તો જીવન એક ક્રીડાસ્થાન છે.\ngujarati Prediction: તાચુ પીતે ત ઈ .છો છ વન છક છરલમત  ્ટળ ીછે.\nEvaluation translation (should we go to the mall?) : ('તમે તમે તમે કોઈ જશે?<END>',)\n-------------------------------------------\nIteration 2000 : 1.5708446502685547\nEnglish: Bhandup West: Ramesh Gajanan Korgaonkar (Shiv Sena)\ngujarati Translation: ભાંડુપ (પશ્ચિમ) - રમેશ કોરગાંવકર (શિવસેના)\ngujarati Prediction: સારડન  અઅા્ચિમ  ક અાાશાકોં્્રડા  અર્વરેના \nEvaluation translation (should we go to the mall?) : ('તમે તમે તમે કોઈ જાણો છો?<END>',)\n-------------------------------------------\nIteration 3000 : 1.3931231498718262\nEnglish: were given.\ngujarati Translation: જ આપવામાં આવ્યા હતા.\ngujarati Prediction: જેકપ્ામાં આવ્યા હતા.\nEvaluation translation (should we go to the mall?) : ('તમે તમે તમે તમે કરવા માટે?<END>',)\n-------------------------------------------\nIteration 4000 : 1.420243263244629\nEnglish: Jesus Christ is the primary figure in the outworking of Gods purpose.\ngujarati Translation: યહોવાહનો હેતુ સિદ્ધ કરવામાં ઈસુ ખ્રિસ્ત મુખ્ય ભૂમિકા ભજવે છે.\ngujarati Prediction: યહોવાહના કતકુ કાવ્ધાકરેાનાં આસુનન્રિસ્તીકાશ્ય કાલિકા ભજવા છે.\nEvaluation translation (should we go to the mall?) : ('તમે તમે કોઈ રીતે જાણતા નથી?<END>',)\n-------------------------------------------\nIteration 5000 : 1.5153506994247437\nEnglish: 300 medical teachers resign\ngujarati Translation: 300 ડોક્ટર્સે આપ્યા રાજીનામા\ngujarati Prediction: આ00 ગિલટટર સ  મવ્યા\nEvaluation translation (should we go to the mall?) : ('તમે તમે કોઈ જાણો છો?<END>',)\n-------------------------------------------\nEpoch 2\n5575\nIteration 0 : 1.46055269241333\nEnglish: But many have tried to supply missing details.\ngujarati Translation: ઘણા જાતજાતની માહિતી અને અફવા ફેલાવે છે.\ngujarati Prediction: પરી મ ણનીનિા સાતિતી અને આનસાનમરરાવવ છે.\nEvaluation translation (should we go to the mall?) : ('તમે તમે તમે કેવી રીતે કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 1000 : 1.4236279726028442\nEnglish: Truly, life is a game.\ngujarati Translation: સાચી રીતે જોઈએ તો જીવન એક ક્રીડાસ્થાન છે.\ngujarati Prediction: તારુ મીતે મ ખ .છે ત વન છક છરયિન .્તળન છે.\nEvaluation translation (should we go to the mall?) : ('તમે તમે કોઈ જાણતા નથી?<END>',)\n-------------------------------------------\nIteration 2000 : 1.5072731971740723\nEnglish: Bhandup West: Ramesh Gajanan Korgaonkar (Shiv Sena)\ngujarati Translation: ભાંડુપ (પશ્ચિમ) - રમેશ કોરગાંવકર (શિવસેના)\ngujarati Prediction: રારડ  ૂસરા્ચિમ  ર રાાશ કરં્્ળઠા  ગરાવા\nEvaluation translation (should we go to the mall?) : ('તમે તમે તમે કોઈ જાણો છો?<END>',)\n-------------------------------------------\nIteration 3000 : 1.315472960472107\nEnglish: were given.\ngujarati Translation: જ આપવામાં આવ્યા હતા.\ngujarati Prediction: જ્હપીામાં આવીયો હતા.\nEvaluation translation (should we go to the mall?) : ('તમે તમે તમે કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 4000 : 1.3894141912460327\nEnglish: Jesus Christ is the primary figure in the outworking of Gods purpose.\ngujarati Translation: યહોવાહનો હેતુ સિદ્ધ કરવામાં ઈસુ ખ્રિસ્ત મુખ્ય ભૂમિકા ભજવે છે.\ngujarati Prediction: ઈહોવાહના સાલુ કાં્ધાપરવા ાં આસુનક્રિસ્તીપાદ્ય પાલિકા ભજવા છે.\nEvaluation translation (should we go to the mall?) : ('તમે તમે કોઈ જાણો છો?<END>',)\n-------------------------------------------\nIteration 5000 : 1.5074816942214966\nEnglish: 300 medical teachers resign\ngujarati Translation: 300 ડોક્ટર્સે આપ્યા રાજીનામા\ngujarati Prediction: 300 ગિલટટરેસ  આપવયા\nEvaluation translation (should we go to the mall?) : ('તમે તમે તમે કોઈ જાણો છો?<END>',)\n-------------------------------------------\nEpoch 3\n5575\nIteration 0 : 1.388749122619629\nEnglish: But many have tried to supply missing details.\ngujarati Translation: ઘણા જાતજાતની માહિતી અને અફવા ફેલાવે છે.\ngujarati Prediction: પણા લ તનેતિી સાતિતી મને તલળા મેલાયવ છે.\nEvaluation translation (should we go to the mall?) : ('તમે કોઈ જાણતા હોય તો તમે જાણવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 1000 : 1.3594279289245605\nEnglish: Truly, life is a game.\ngujarati Translation: સાચી રીતે જોઈએ તો જીવન એક ક્રીડાસ્થાન છે.\ngujarati Prediction: હાથો આીતે મ ખ .છો મ વ  છક છહયિન  ્ટ્ ોછે.\nEvaluation translation (should we go to the mall?) : ('તે કોઈ જાણો છો?<END>',)\n-------------------------------------------\nIteration 2000 : 1.4694859981536865\nEnglish: Bhandup West: Ramesh Gajanan Korgaonkar (Shiv Sena)\ngujarati Translation: ભાંડુપ (પશ્ચિમ) - રમેશ કોરગાંવકર (શિવસેના)\ngujarati Prediction: રારડ રરરકી્રિમ  અ કાેશ કોર્નનઠાર રર્વસેના  ..................... .................  .. ....    ... . ....  ..  . ...................................................................... ................\nEvaluation translation (should we go to the mall?) : ('તમે તેઓ કોઈ પણ કરવા માટે?<END>',)\n-------------------------------------------\nIteration 3000 : 1.2711735963821411\nEnglish: were given.\ngujarati Translation: જ આપવામાં આવ્યા હતા.\ngujarati Prediction: જ્કપ્ામાં આવીયુ હતા.\nEvaluation translation (should we go to the mall?) : ('તમે તમે તમે કેવી રીતે કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 4000 : 1.353399395942688\nEnglish: Jesus Christ is the primary figure in the outworking of Gods purpose.\ngujarati Translation: યહોવાહનો હેતુ સિદ્ધ કરવામાં ઈસુ ખ્રિસ્ત મુખ્ય ભૂમિકા ભજવે છે.\ngujarati Prediction: જહોવાહના સિતુ કાન્ધાપરવા ાં આશુનખ્રિસ્તીકાખ્ય પામિકા કાવી છે.\nEvaluation translation (should we go to the mall?) : ('તમે કોઈ જાણતા નથી?<END>',)\n-------------------------------------------\nIteration 5000 : 1.4356892108917236\nEnglish: 300 medical teachers resign\ngujarati Translation: 300 ડોક્ટર્સે આપ્યા રાજીનામા\ngujarati Prediction: 300 ગિલ્ટરોસ  કગવયુ\nEvaluation translation (should we go to the mall?) : ('તમે તમે કોઈ જાણતા હશે?<END>',)\n-------------------------------------------\nEpoch 4\n5575\nIteration 0 : 1.3693679571151733\nEnglish: But many have tried to supply missing details.\ngujarati Translation: ઘણા જાતજાતની માહિતી અને અફવા ફેલાવે છે.\ngujarati Prediction: પણા લ યે તિે સાતિતી મને સલસાદમાલાવવ છે.\nEvaluation translation (should we go to the mall?) : ('શું તમે કેવી રીતે કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 1000 : 1.3469960689544678\nEnglish: Truly, life is a game.\ngujarati Translation: સાચી રીતે જોઈએ તો જીવન એક ક્રીડાસ્થાન છે.\ngujarati Prediction: તારુ છીતે ત ઈ .છે છ તન છક છહયિમ  ીટળ  છે.\nEvaluation translation (should we go to the mall?) : ('તે કોઈ પણ કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 2000 : 1.4272879362106323\nEnglish: Bhandup West: Ramesh Gajanan Korgaonkar (Shiv Sena)\ngujarati Translation: ભાંડુપ (પશ્ચિમ) - રમેશ કોરગાંવકર (શિવસેના)\ngujarati Prediction: બારડા  રકા્ચિમ  ર રાેશ કરં્્ર  ર સરરવસે)ા)   .            .    . ....    .   . .         .              ..         ..    .      ............. ..... . ........  .......... ......     .   ..   ........ \nEvaluation translation (should we go to the mall?) : ('તેઓ કોઈ પણ તેને કોઈ જાય છે?<END>',)\n-------------------------------------------\nIteration 3000 : 1.2477587461471558\nEnglish: were given.\ngujarati Translation: જ આપવામાં આવ્યા હતા.\ngujarati Prediction: જ્હવવામાં આવ્યા હતા.\nEvaluation translation (should we go to the mall?) : ('તમે તે કોઈ જાય છે?<END>',)\n-------------------------------------------\nIteration 4000 : 1.3108395338058472\nEnglish: Jesus Christ is the primary figure in the outworking of Gods purpose.\ngujarati Translation: યહોવાહનો હેતુ સિદ્ધ કરવામાં ઈસુ ખ્રિસ્ત મુખ્ય ભૂમિકા ભજવે છે.\ngujarati Prediction: ઈહોવાહની સિતુ કાન્ધાપરવા ાં આશુ ખ્યિસ્તીજાશ્ય પામિકા ભજવવ છે.\nEvaluation translation (should we go to the mall?) : ('તમે તમે કોઈ જાય છે?<END>',)\n-------------------------------------------\nIteration 5000 : 1.4039524793624878\nEnglish: 300 medical teachers resign\ngujarati Translation: 300 ડોક્ટર્સે આપ્યા રાજીનામા\ngujarati Prediction: 300 લિલ્યરોસ  આપવયા હોખ્નામાં.. .. . ...  .. ..   ...  . . .  .. ....  ....  .... .   .        .    .    ..   .    ..  ... . .  ............. ........ . .................. ...... ..... ...............\nEvaluation translation (should we go to the mall?) : ('તમે તેઓ કોઈ જાણો છો?<END>',)\n-------------------------------------------\nEpoch 5\n5575\nIteration 0 : 1.3515838384628296\nEnglish: But many have tried to supply missing details.\ngujarati Translation: ઘણા જાતજાતની માહિતી અને અફવા ફેલાવે છે.\ngujarati Prediction: પણા મ તન તીે સાતિતી મને તનસા માલાવી છે.\nEvaluation translation (should we go to the mall?) : ('શું તમે કેવી રીતે કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 1000 : 1.303220510482788\nEnglish: Truly, life is a game.\ngujarati Translation: સાચી રીતે જોઈએ તો જીવન એક ક્રીડાસ્થાન છે.\ngujarati Prediction: તારી સીતે ત ખએ.છે ત વન છક છરષિ    ટળ ેછે.\nEvaluation translation (should we go to the mall?) : ('તે કોઈ જાણો છો?<END>',)\n-------------------------------------------\nIteration 2000 : 1.4049365520477295\nEnglish: Bhandup West: Ramesh Gajanan Korgaonkar (Shiv Sena)\ngujarati Translation: ભાંડુપ (પશ્ચિમ) - રમેશ કોરગાંવકર (શિવસેના)\ngujarati Prediction: રારડ ર રરા્ચિમ  ર રાવશ :ોં્્નધ  )અરાવસેના)\nEvaluation translation (should we go to the mall?) : ('તમે તેમને કોઈ જાય છે?<END>',)\n-------------------------------------------\nIteration 3000 : 1.2115440368652344\nEnglish: were given.\ngujarati Translation: જ આપવામાં આવ્યા હતા.\ngujarati Prediction: જેહવ્ામાં આવ્યા હતા.\nEvaluation translation (should we go to the mall?) : ('તેઓ તેને કોઈ જ કરવાની જરૂર છે?<END>',)\n-------------------------------------------\nIteration 4000 : 1.2915469408035278\nEnglish: Jesus Christ is the primary figure in the outworking of Gods purpose.\ngujarati Translation: યહોવાહનો હેતુ સિદ્ધ કરવામાં ઈસુ ખ્રિસ્ત મુખ્ય ભૂમિકા ભજવે છે.\ngujarati Prediction: ઈહોવાહના પિતુ કાન્ધાપરવા ાં આસુનખ્રિસ્તીકાજ્ય કાલિકા ભજના છે.\nEvaluation translation (should we go to the mall?) : ('તમે કોઈ જાણતા હોય તો ?<END>',)\n-------------------------------------------\nIteration 5000 : 1.3822944164276123\nEnglish: 300 medical teachers resign\ngujarati Translation: 300 ડોક્ટર્સે આપ્યા રાજીનામા\ngujarati Prediction: 30  મિક્યરોસ લમરીયા હાજ્નામાં                                                                                                           .                                                               \nEvaluation translation (should we go to the mall?) : ('તમે તેમને કોઈ જ કરવા માંગો છો?<END>',)\n-------------------------------------------\nEpoch 6\n5575\nIteration 0 : 1.3181301355361938\nEnglish: But many have tried to supply missing details.\ngujarati Translation: ઘણા જાતજાતની માહિતી અને અફવા ફેલાવે છે.\ngujarati Prediction: પણા બ તેાત ે સાતિતી મને તનવાદમરલાવવ છે.\nEvaluation translation (should we go to the mall?) : ('શું તમે કેવી રીતે કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 1000 : 1.2818983793258667\nEnglish: Truly, life is a game.\ngujarati Translation: સાચી રીતે જોઈએ તો જીવન એક ક્રીડાસ્થાન છે.\ngujarati Prediction: તાથુ તહતે,ત ઈ .છે મ વી છક છહયિમ  ્ત્ ીછે.\nEvaluation translation (should we go to the mall?) : ('તે કોઈ જશે?<END>',)\n-------------------------------------------\nIteration 2000 : 1.3487741947174072\nEnglish: Bhandup West: Ramesh Gajanan Korgaonkar (Shiv Sena)\ngujarati Translation: ભાંડુપ (પશ્ચિમ) - રમેશ કોરગાંવકર (શિવસેના)\ngujarati Prediction: સારડનર રરા્ચિમ  ર રાાશ ગપર્્નડાર સર્વસેના)\nEvaluation translation (should we go to the mall?) : ('તે શું તમે શું કરવું?<END>',)\n-------------------------------------------\nIteration 3000 : 1.160676121711731\nEnglish: were given.\ngujarati Translation: જ આપવામાં આવ્યા હતા.\ngujarati Prediction: જાહવીામાં આવ્યા હતા.\nEvaluation translation (should we go to the mall?) : ('તમે તેને કોઈ જ કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 4000 : 1.2527756690979004\nEnglish: Jesus Christ is the primary figure in the outworking of Gods purpose.\ngujarati Translation: યહોવાહનો હેતુ સિદ્ધ કરવામાં ઈસુ ખ્રિસ્ત મુખ્ય ભૂમિકા ભજવે છે.\ngujarati Prediction: ઈહોવાહના સાતુ પાન્ધાપરવાથાં આશુનખ્રિસ્તીછાશ્ય પાલિકા ભજવી છે.\nEvaluation translation (should we go to the mall?) : ('તમે કોઈ પણ કરવા માટે?<END>',)\n-------------------------------------------\nIteration 5000 : 1.3314971923828125\nEnglish: 300 medical teachers resign\ngujarati Translation: 300 ડોક્ટર્સે આપ્યા રાજીનામા\ngujarati Prediction: 300 ગિલ્ટર સ  આતવયા છાખ્ના ાં                                                                                                                                                                           \nEvaluation translation (should we go to the mall?) : ('તમે કોઈ જાણે છે?<END>',)\n-------------------------------------------\nEpoch 7\n5575\nIteration 0 : 1.2789078950881958\nEnglish: But many have tried to supply missing details.\ngujarati Translation: ઘણા જાતજાતની માહિતી અને અફવા ફેલાવે છે.\ngujarati Prediction: પણા લ તેાતિે સાતિતી અને તનસાઓમાલાવો છે.\nEvaluation translation (should we go to the mall?) : ('શું તમે કેમ કરવા માટે?<END>',)\n-------------------------------------------\nIteration 1000 : 1.2689974308013916\nEnglish: Truly, life is a game.\ngujarati Translation: સાચી રીતે જોઈએ તો જીવન એક ક્રીડાસ્થાન છે.\ngujarati Prediction: તાચે જીતે,ત ય .છે ત વ  છક મરષિસ  .ત્  છે.\nEvaluation translation (should we go to the mall?) : ('તે તમે કોઈ પણ કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 2000 : 1.3478856086730957\nEnglish: Bhandup West: Ramesh Gajanan Korgaonkar (Shiv Sena)\ngujarati Translation: ભાંડુપ (પશ્ચિમ) - રમેશ કોરગાંવકર (શિવસેના)\ngujarati Prediction: રાનડવ  રરો્ચિમ  ર રાણશ કપરિ્ળઠા  સર્વરેના)\nEvaluation translation (should we go to the mall?) : ('તે શું તમે શું કરવું?<END>',)\n-------------------------------------------\nIteration 3000 : 1.1693580150604248\nEnglish: were given.\ngujarati Translation: જ આપવામાં આવ્યા હતા.\ngujarati Prediction: ક્કપીામાં આવ્યુ હતા.\nEvaluation translation (should we go to the mall?) : ('તે તમે તે કેટલાક કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 4000 : 1.241734266281128\nEnglish: Jesus Christ is the primary figure in the outworking of Gods purpose.\ngujarati Translation: યહોવાહનો હેતુ સિદ્ધ કરવામાં ઈસુ ખ્રિસ્ત મુખ્ય ભૂમિકા ભજવે છે.\ngujarati Prediction: ઈહોવાહના સિતુ જાર્ધાસરવા ાં આસુએપ્રિસ્તીજાખ્ય પાલિકા ભજવા છે.\nEvaluation translation (should we go to the mall?) : ('તે કેટલાક કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 5000 : 1.3031295537948608\nEnglish: 300 medical teachers resign\ngujarati Translation: 300 ડોક્ટર્સે આપ્યા રાજીનામા\ngujarati Prediction: 300 મિલ્ટર સ  કરીયા હેખ્નામાં                                                                                                                                                                           \nEvaluation translation (should we go to the mall?) : ('તમે તેમને કોઈ જ કરવા માંગો છો?<END>',)\n-------------------------------------------\nEpoch 8\n5575\nIteration 0 : 1.2528702020645142\nEnglish: But many have tried to supply missing details.\ngujarati Translation: ઘણા જાતજાતની માહિતી અને અફવા ફેલાવે છે.\ngujarati Prediction: પણા વ ણેનતને સાતિતી સને તલસા માલા વ છે.\nEvaluation translation (should we go to the mall?) : ('તેમને કોઈ પણ કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 1000 : 1.2220211029052734\nEnglish: Truly, life is a game.\ngujarati Translation: સાચી રીતે જોઈએ તો જીવન એક ક્રીડાસ્થાન છે.\ngujarati Prediction: હાંુ છહતે,સ ઈ  છે મીવી છક છરરિમ  ્તા ોછે.\nEvaluation translation (should we go to the mall?) : ('તેમને કોઈ પણ કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 2000 : 1.324165940284729\nEnglish: Bhandup West: Ramesh Gajanan Korgaonkar (Shiv Sena)\ngujarati Translation: ભાંડુપ (પશ્ચિમ) - રમેશ કોરગાંવકર (શિવસેના)\ngujarati Prediction: ભુજડુર (રુ્ચિમ  ર રાનશ અપરિ ળગાર સ ાવસેન))\nEvaluation translation (should we go to the mall?) : ('તેમને કોઈ પણ કરવા માટે?<END>',)\n-------------------------------------------\nIteration 3000 : 1.1245386600494385\nEnglish: were given.\ngujarati Translation: જ આપવામાં આવ્યા હતા.\ngujarati Prediction: જાહવીામાં આવીયુ હતા.\nEvaluation translation (should we go to the mall?) : ('તે કોઈ સમય માટે જરૂર છે?<END>',)\n-------------------------------------------\nIteration 4000 : 1.2028414011001587\nEnglish: Jesus Christ is the primary figure in the outworking of Gods purpose.\ngujarati Translation: યહોવાહનો હેતુ સિદ્ધ કરવામાં ઈસુ ખ્રિસ્ત મુખ્ય ભૂમિકા ભજવે છે.\ngujarati Prediction: ઈહોવાહના સિતુ કાદ્ધાપરવાથાં આશુનખ્રિસ્ત પાદ્ય પાલિકા ભજવશ છે.\nEvaluation translation (should we go to the mall?) : ('તમે તે કેવી રીતે કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 5000 : 1.2853355407714844\nEnglish: 300 medical teachers resign\ngujarati Translation: 300 ડોક્ટર્સે આપ્યા રાજીનામા\ngujarati Prediction: 300 કિલ્ટર સ  મરીયા છેખીનામાં                                                                                                                           ન                               .               \nEvaluation translation (should we go to the mall?) : ('તમે કોઈ જ શું છે?<END>',)\n-------------------------------------------\nEpoch 9\n5575\nIteration 0 : 1.2433805465698242\nEnglish: But many have tried to supply missing details.\ngujarati Translation: ઘણા જાતજાતની માહિતી અને અફવા ફેલાવે છે.\ngujarati Prediction: પણા મ ણેાતીે સાતિતી અનેકપનસાઓમાલાવી છે.\nEvaluation translation (should we go to the mall?) : ('તેમને કોઈ પણ કરવા માટે જોઈએ?<END>',)\n-------------------------------------------\nIteration 1000 : 1.1970134973526\nEnglish: Truly, life is a game.\ngujarati Translation: સાચી રીતે જોઈએ તો જીવન એક ક્રીડાસ્થાન છે.\ngujarati Prediction: આાચો જીતે મીય  છે મ વ. છવ છરષિમ  .ટ   છે.\nEvaluation translation (should we go to the mall?) : ('તેમને કોઈ પણ કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 2000 : 1.292429804801941\nEnglish: Bhandup West: Ramesh Gajanan Korgaonkar (Shiv Sena)\ngujarati Translation: ભાંડુપ (પશ્ચિમ) - રમેશ કોરગાંવકર (શિવસેના)\ngujarati Prediction: બાજડૂરમ(રા્ચિમ  સ રાાશ કપરો્રક રીસસ્વાેના)                                                                                                                                                              \nEvaluation translation (should we go to the mall?) : ('તેમને કોઈ પણ કરવા માંગો છો?<END>',)\n-------------------------------------------\nIteration 3000 : 1.1171683073043823\nEnglish: were given.\ngujarati Translation: જ આપવામાં આવ્યા હતા.\ngujarati Prediction: સેરગ્ામાં આવીયા હતા.\nEvaluation translation (should we go to the mall?) : ('તેમને કોઈ પણ કરવાનું છે?<END>',)\n-------------------------------------------\nIteration 4000 : 1.1826307773590088\nEnglish: Jesus Christ is the primary figure in the outworking of Gods purpose.\ngujarati Translation: યહોવાહનો હેતુ સિદ્ધ કરવામાં ઈસુ ખ્રિસ્ત મુખ્ય ભૂમિકા ભજવે છે.\ngujarati Prediction: ઈહોવાનના સાતુ તાસ્ધિપરવા ાં આસુનખ્રિસ્ત પાદ્ય પામિકા ભજવા છે.\nEvaluation translation (should we go to the mall?) : ('તમે તેને કોઈ જ કરવા માટે?<END>',)\n-------------------------------------------\nIteration 5000 : 1.2412182092666626\nEnglish: 300 medical teachers resign\ngujarati Translation: 300 ડોક્ટર્સે આપ્યા રાજીનામા\ngujarati Prediction: 300 મેલ્ટર સ  કવીયા હેખ્નામું                                                                                                                                                  .                   .    \nEvaluation translation (should we go to the mall?) : ('તમે મારી માટે તે મારી નથી?<END>',)\n-------------------------------------------\nCPU times: user 3h 8min 27s, sys: 24.9 s, total: 3h 8min 52s\nWall time: 2h 11min 12s\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer.eval()\ndef translate(eng_sentence):\n    eng_sentence = (eng_sentence,)\n    kn_sentence = (\"\",)\n    for word_counter in range(max_sequence_length):\n        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)\n        predictions = transformer(eng_sentence,\n                                  kn_sentence,\n                                  encoder_self_attention_mask.to(device), \n                                  decoder_self_attention_mask.to(device), \n                                  decoder_cross_attention_mask.to(device),\n                                  enc_start_token=False,\n                                  enc_end_token=False,\n                                  dec_start_token=True,\n                                  dec_end_token=False)\n        next_token_prob_distribution = predictions[0][word_counter]\n        next_token_index = torch.argmax(next_token_prob_distribution).item()\n        next_token = index_to_gujarati[next_token_index]\n        kn_sentence = (kn_sentence[0] + next_token, )\n        if next_token == END_TOKEN:\n            break\n    return kn_sentence[0]","metadata":{"execution":{"iopub.status.busy":"2024-07-06T09:19:00.941706Z","iopub.execute_input":"2024-07-06T09:19:00.942351Z","iopub.status.idle":"2024-07-06T09:19:00.950659Z","shell.execute_reply.started":"2024-07-06T09:19:00.942320Z","shell.execute_reply":"2024-07-06T09:19:00.949605Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"translation = translate(\"Don't do that\")\nprint(translation)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T09:26:33.342228Z","iopub.execute_input":"2024-07-06T09:26:33.342899Z","iopub.status.idle":"2024-07-06T09:26:33.563955Z","shell.execute_reply.started":"2024-07-06T09:26:33.342865Z","shell.execute_reply":"2024-07-06T09:26:33.563131Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"તે કરવાની જરૂર ન કરો.<END>\n","output_type":"stream"}]},{"cell_type":"code","source":"translation = translate(\"where are you?\")\nprint(translation)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T09:28:24.176840Z","iopub.execute_input":"2024-07-06T09:28:24.177211Z","iopub.status.idle":"2024-07-06T09:28:24.316068Z","shell.execute_reply.started":"2024-07-06T09:28:24.177182Z","shell.execute_reply":"2024-07-06T09:28:24.315081Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"તમે ક્યાં છે?<END>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Meaning : where are you?","metadata":{}},{"cell_type":"code","source":"translation = translate(\"who are you?\")\nprint(translation)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T09:29:34.553062Z","iopub.execute_input":"2024-07-06T09:29:34.553647Z","iopub.status.idle":"2024-07-06T09:29:34.722057Z","shell.execute_reply.started":"2024-07-06T09:29:34.553618Z","shell.execute_reply":"2024-07-06T09:29:34.721101Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"કોણ છે આ કોણ છે?<END>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Meaning : who is who is this?**","metadata":{}},{"cell_type":"code","source":"translation = translate(\"Please check it\")\nprint(translation)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T09:30:50.720659Z","iopub.execute_input":"2024-07-06T09:30:50.721063Z","iopub.status.idle":"2024-07-06T09:30:50.873981Z","shell.execute_reply.started":"2024-07-06T09:30:50.721031Z","shell.execute_reply":"2024-07-06T09:30:50.873037Z"},"trusted":true},"execution_count":126,"outputs":[{"name":"stdout","text":"કાળજી પાસે કરો<END>\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sentencepiece as spm\n\n# Example Gujarati text data\ntrain_data = \"\"\"\nગુજરાતી વાક્યનું ઉદાહરણ છે. આ ઘણું સરસ છે.\nબીજું ઉદાહરણ અહીં છે. ત્રીજું ઉદાહરણ પણ છે.\n\"\"\"\n\n# Save the training data to a file\nwith open('gujarati_text.txt', 'w', encoding='utf-8') as f:\n    f.write(train_data)\n\n# Train SentencePiece model with a target vocabulary size\nvocab_size = 2000  # You can adjust this size as needed\n\nspm.SentencePieceTrainer.train(input='gujarati_text.txt', model_prefix='gujarati', vocab_size=vocab_size, character_coverage=1.0, model_type='bpe')\n\n# Load the trained model\nsp = spm.SentencePieceProcessor()\nsp.load('gujarati.model')\n\n# Test the tokenizer\ntest_sentence = \"આ એક ઉદાહરણ છે.\"\ntokens = sp.encode_as_pieces(test_sentence)\ntoken_ids = sp.encode_as_ids(test_sentence)\n\nprint(\"Tokens:\", tokens)\nprint(\"Token IDs:\", token_ids)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:20:45.764077Z","iopub.status.idle":"2024-07-06T05:20:45.764620Z","shell.execute_reply.started":"2024-07-06T05:20:45.764345Z","shell.execute_reply":"2024-07-06T05:20:45.764366Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scaled_dot_product(q, k, v, mask=None):\n    d_k = q.size()[-1]\n    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n    if mask is not None:\n        scaled = scaled.permute(1, 0, 2, 3) + mask\n        scaled = scaled.permute(1, 0, 2, 3)\n    attention = F.softmax(scaled, dim=-1)\n    values = torch.matmul(attention, v)\n    return values, attention","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:20:53.800570Z","iopub.execute_input":"2024-07-06T05:20:53.800934Z","iopub.status.idle":"2024-07-06T05:20:53.807680Z","shell.execute_reply.started":"2024-07-06T05:20:53.800904Z","shell.execute_reply":"2024-07-06T05:20:53.806711Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_sequence_length):\n        super().__init__()\n        self.max_sequence_length = max_sequence_length\n        self.d_model = d_model\n\n    def forward(self):\n        even_i = torch.arange(0, self.d_model, 2).float()\n        denominator = torch.pow(10000, even_i/self.d_model)\n        position = (torch.arange(self.max_sequence_length)\n                          .reshape(self.max_sequence_length, 1))\n        even_PE = torch.sin(position / denominator)\n        odd_PE = torch.cos(position / denominator)\n        stacked = torch.stack([even_PE, odd_PE], dim=2)\n        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n        return PE","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:20:53.809336Z","iopub.execute_input":"2024-07-06T05:20:53.809690Z","iopub.status.idle":"2024-07-06T05:20:53.819422Z","shell.execute_reply.started":"2024-07-06T05:20:53.809666Z","shell.execute_reply":"2024-07-06T05:20:53.818708Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"class SentenceEmbedding(nn.Module):\n    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n        super().__init__()\n        self.vocab_size = len(language_to_index)\n        self.max_sequence_length = max_sequence_length\n        self.embedding = nn.Embedding(self.vocab_size, d_model)\n        self.language_to_index = language_to_index\n        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n        self.dropout = nn.Dropout(p=0.1)\n        self.START_TOKEN = START_TOKEN\n        self.END_TOKEN = END_TOKEN\n        self.PADDING_TOKEN = PADDING_TOKEN\n    \n    def batch_tokenize(self, batch, start_token, end_token):\n        def tokenize(sentence, start_token, end_token):\n            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n            if start_token:\n                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n            if end_token:\n                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n            return torch.tensor(sentence_word_indicies)\n\n        tokenized = []\n        for sentence_num in range(len(batch)):\n            tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n        tokenized = torch.stack(tokenized)\n        return tokenized.to(device)\n    \n    def forward(self, x, start_token, end_token): # sentence\n        x = self.batch_tokenize(x, start_token, end_token)\n        x = self.embedding(x)\n        pos = self.position_encoder().to(device)\n        x = self.dropout(x + pos)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:20:53.820599Z","iopub.execute_input":"2024-07-06T05:20:53.820963Z","iopub.status.idle":"2024-07-06T05:20:53.834249Z","shell.execute_reply.started":"2024-07-06T05:20:53.820934Z","shell.execute_reply":"2024-07-06T05:20:53.833458Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n        self.linear_layer = nn.Linear(d_model, d_model)\n    \n    def forward(self, x, mask):\n        batch_size, sequence_length, d_model = x.size()\n        qkv = self.qkv_layer(x)\n        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n        qkv = qkv.permute(0, 2, 1, 3)\n        q, k, v = qkv.chunk(3, dim=-1)\n        values, attention = scaled_dot_product(q, k, v, mask)\n        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n        out = self.linear_layer(values)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:20:53.835527Z","iopub.execute_input":"2024-07-06T05:20:53.835951Z","iopub.status.idle":"2024-07-06T05:20:53.848213Z","shell.execute_reply.started":"2024-07-06T05:20:53.835921Z","shell.execute_reply":"2024-07-06T05:20:53.847447Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n    def __init__(self, parameters_shape, eps=1e-5):\n        super().__init__()\n        self.parameters_shape=parameters_shape\n        self.eps=eps\n        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n\n    def forward(self, inputs):\n        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n        mean = inputs.mean(dim=dims, keepdim=True)\n        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n        std = (var + self.eps).sqrt()\n        y = (inputs - mean) / std\n        out = self.gamma * y + self.beta\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:20:53.850522Z","iopub.execute_input":"2024-07-06T05:20:53.851147Z","iopub.status.idle":"2024-07-06T05:20:53.862031Z","shell.execute_reply.started":"2024-07-06T05:20:53.851115Z","shell.execute_reply":"2024-07-06T05:20:53.861341Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, hidden, drop_prob=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.linear1 = nn.Linear(d_model, hidden)\n        self.linear2 = nn.Linear(hidden, d_model)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=drop_prob)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:20:53.863150Z","iopub.execute_input":"2024-07-06T05:20:53.863497Z","iopub.status.idle":"2024-07-06T05:20:53.873276Z","shell.execute_reply.started":"2024-07-06T05:20:53.863467Z","shell.execute_reply":"2024-07-06T05:20:53.872517Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n        super(EncoderLayer, self).__init__()\n        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout1 = nn.Dropout(p=drop_prob)\n        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout2 = nn.Dropout(p=drop_prob)\n\n    def forward(self, x, self_attention_mask):\n        residual_x = x.clone()\n        x = self.attention(x, mask=self_attention_mask)\n        x = self.dropout1(x)\n        x = self.norm1(x + residual_x)\n        residual_x = x.clone()\n        x = self.ffn(x)\n        x = self.dropout2(x)\n        x = self.norm2(x + residual_x)\n        return x # 30 x 200 x 512","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:20:53.874381Z","iopub.execute_input":"2024-07-06T05:20:53.874751Z","iopub.status.idle":"2024-07-06T05:20:53.884536Z","shell.execute_reply.started":"2024-07-06T05:20:53.874718Z","shell.execute_reply":"2024-07-06T05:20:53.883681Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"class SequentialEncoder(nn.Sequential):\n    def forward(self, *inputs):\n        x, self_attention_mask  = inputs\n        for module in self._modules.values():\n            x = module(x, self_attention_mask)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:20:53.885781Z","iopub.execute_input":"2024-07-06T05:20:53.886632Z","iopub.status.idle":"2024-07-06T05:20:53.898525Z","shell.execute_reply.started":"2024-07-06T05:20:53.886600Z","shell.execute_reply":"2024-07-06T05:20:53.897732Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, \n                 d_model, \n                 ffn_hidden, \n                 num_heads, \n                 drop_prob, \n                 num_layers,\n                 max_sequence_length,\n                 language_to_index,\n                 START_TOKEN,\n                 END_TOKEN, \n                 PADDING_TOKEN):\n        super().__init__()\n        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n                                      for _ in range(num_layers)])\n\n    def forward(self, x, self_attention_mask, start_token, end_token):\n        x = self.sentence_embedding(x, start_token, end_token)\n        x = self.layers(x, self_attention_mask)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:20:53.918352Z","iopub.execute_input":"2024-07-06T05:20:53.918681Z","iopub.status.idle":"2024-07-06T05:20:53.926494Z","shell.execute_reply.started":"2024-07-06T05:20:53.918659Z","shell.execute_reply":"2024-07-06T05:20:53.925447Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"class MultiHeadCrossAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n        self.q_layer = nn.Linear(d_model , d_model)\n        self.linear_layer = nn.Linear(d_model, d_model)\n    \n    def forward(self, x, y, mask):\n        batch_size, sequence_length, d_model = x.size() # in practice, this is the same for both languages...so we can technically combine with normal attention\n        kv = self.kv_layer(x)\n        q = self.q_layer(y)\n        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n        kv = kv.permute(0, 2, 1, 3)\n        q = q.permute(0, 2, 1, 3)\n        k, v = kv.chunk(2, dim=-1)\n        values, attention = scaled_dot_product(q, k, v, mask) # We don't need the mask for cross attention, removing in outer function!\n        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model)\n        out = self.linear_layer(values)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:20:53.928139Z","iopub.execute_input":"2024-07-06T05:20:53.928437Z","iopub.status.idle":"2024-07-06T05:20:53.940458Z","shell.execute_reply.started":"2024-07-06T05:20:53.928384Z","shell.execute_reply":"2024-07-06T05:20:53.939700Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n        super(DecoderLayer, self).__init__()\n        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout1 = nn.Dropout(p=drop_prob)\n\n        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout2 = nn.Dropout(p=drop_prob)\n\n        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n        self.dropout3 = nn.Dropout(p=drop_prob)\n\n    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n        _y = y.clone()\n        y = self.self_attention(y, mask=self_attention_mask)\n        y = self.dropout1(y)\n        y = self.layer_norm1(y + _y)\n\n        _y = y.clone()\n        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n        y = self.dropout2(y)\n        y = self.layer_norm2(y + _y)\n\n        _y = y.clone()\n        y = self.ffn(y)\n        y = self.dropout3(y)\n        y = self.layer_norm3(y + _y)\n        return y","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:20:53.941477Z","iopub.execute_input":"2024-07-06T05:20:53.941736Z","iopub.status.idle":"2024-07-06T05:20:53.955280Z","shell.execute_reply.started":"2024-07-06T05:20:53.941715Z","shell.execute_reply":"2024-07-06T05:20:53.954454Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"class SequentialDecoder(nn.Sequential):\n    def forward(self, *inputs):\n        x, y, self_attention_mask, cross_attention_mask = inputs\n        for module in self._modules.values():\n            y = module(x, y, self_attention_mask, cross_attention_mask)\n        return y","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:20:53.957128Z","iopub.execute_input":"2024-07-06T05:20:53.957392Z","iopub.status.idle":"2024-07-06T05:20:53.969238Z","shell.execute_reply.started":"2024-07-06T05:20:53.957370Z","shell.execute_reply":"2024-07-06T05:20:53.968453Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, \n                 d_model, \n                 ffn_hidden, \n                 num_heads, \n                 drop_prob, \n                 num_layers,\n                 max_seq_len,\n                 language_to_index,\n                 START_TOKEN,\n                 END_TOKEN, \n                 PADDING_TOKEN):\n        super().__init__()\n        self.sentence_embedding = SentenceEmbedding(max_seq_len, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n\n    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n        y = self.sentence_embedding(y, start_token, end_token)\n        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n        return y","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:20:53.970267Z","iopub.execute_input":"2024-07-06T05:20:53.970631Z","iopub.status.idle":"2024-07-06T05:20:53.980289Z","shell.execute_reply.started":"2024-07-06T05:20:53.970599Z","shell.execute_reply":"2024-07-06T05:20:53.979546Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"\nclass Transformer(nn.Module):\n    def __init__(self, \n                d_model, \n                ffn_hidden, \n                num_heads, \n                drop_prob, \n                num_layers,\n                max_seq_len, \n                guj_vocab_size,\n                english_to_index,\n                gujarati_to_index,\n                START_TOKEN, \n                END_TOKEN, \n                PADDING_TOKEN\n                ):\n        super().__init__()\n        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_seq_len, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_seq_len, gujarati_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n        self.linear = nn.Linear(d_model, guj_vocab_size)\n\n    def forward(self, \n                x, \n                y, \n                encoder_self_attention_mask=None, \n                decoder_self_attention_mask=None, \n                decoder_cross_attention_mask=None,\n                enc_start_token=False,\n                enc_end_token=False,\n                dec_start_token=False, # We should make this true\n                dec_end_token=False): # x, y are batch of sentences\n        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n        out = self.linear(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:20:53.981287Z","iopub.execute_input":"2024-07-06T05:20:53.981564Z","iopub.status.idle":"2024-07-06T05:20:53.994933Z","shell.execute_reply.started":"2024-07-06T05:20:53.981542Z","shell.execute_reply":"2024-07-06T05:20:53.994084Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}